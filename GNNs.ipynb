{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MlFlxfL5dgn2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch_geometric.nn as pyg_nn\n",
        "import torch_geometric.utils as pyg_utils\n",
        "\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch_geometric.datasets import TUDataset, FakeDataset, GNNBenchmarkDataset\n",
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "import torch_geometric.transforms as T\n",
        "\n",
        "from tensorboardX import SummaryWriter\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set verbosity\n",
        "VERBOSE = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Debug tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_edge_index(edge_index):\n",
        "    print('Edges info:')\n",
        "    print('Num edges=', edge_index.size(1))\n",
        "    print(''.join(['edge #' + str(i) + ':\\t' + str(edge_index[0,i].item()) + '->' + str(edge_index[1,i].item()) + ('\\n' if (i+1) % 4 == 0 else ';\\t\\t') for i in range(edge_index.size(1))]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCgqxSiq6I4B"
      },
      "source": [
        "# Defining the custom graph convolution layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8l8hy4NSvu7J"
      },
      "source": [
        "Here we define a custom instance of MessagePassing. It defines a single layer of graph convolution, which can be decomposed into:\n",
        "* Message computation\n",
        "* Aggregation\n",
        "* Update\n",
        "* Pooling\n",
        "\n",
        "Here we give an example of how to subclass the pytorch geometric MessagePassing class to derive a new model (rather than using existing GCNConv and GINConv).\n",
        "\n",
        "We make use of `MessagePassing`'s key building blocks:\n",
        "- `aggr`: The aggregation method to use (\"add\", \"mean\" or \"max\").\n",
        "- `propagate()`: The initial call to start propagating messages. Takes in the edge indices and any other data to pass along (e.g. to update node embeddings).\n",
        "- `message()`: Constructs messages to node i. Takes any argument which was initially passed to propagate().\n",
        "- `update()`: Updates node embeddings. Takes in the output of aggregation as first argument and any argument which was initially passed to propagate().\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "V_0yhAPgvttr"
      },
      "outputs": [],
      "source": [
        "class CustomConv(pyg_nn.MessagePassing):\n",
        "    def __init__(self, in_channels, out_channels, max_neighbors_to_consider=4):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        max_neighbors_to_consider : int\n",
        "            The number of neighbor nodes (from the epsilon-environment) to consider during aggregation.\n",
        "        \"\"\"\n",
        "        super(CustomConv, self).__init__(aggr='mean')\n",
        "        self.out_channels = out_channels\n",
        "        self.max_neighbors_to_consider = max_neighbors_to_consider\n",
        "\n",
        "        # Linear layer applied on the features of the center node.\n",
        "        self.lin_self = nn.Linear(in_channels, out_channels)\n",
        "\n",
        "        # For each neighbor node (from the epsilon-environment) we consider during aggregation, we apply linear layer on its features.\n",
        "        self.lin = nn.ModuleList()\n",
        "        for i in range(max_neighbors_to_consider):\n",
        "            self.lin.append(nn.Linear(in_channels, out_channels))\n",
        "            nn.init.normal_(self.lin[i].weight, mean=0, std=1.0)\n",
        "\n",
        "        # The epsilon that defines the \"eplison environment\" we consider for each node.\n",
        "        # This is a trainable parameter.\n",
        "        self.eps = torch.nn.Parameter(torch.rand(1)) \n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        \"\"\"\n",
        "        Forward computation.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : torch.Tensor\n",
        "            Input node embeddings. Has shape [NUM_NODES, in_channels]\n",
        "        edge_index : torch.Tensor\n",
        "            Edge index. Has shape [2, NUM_EDGES]\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        h : torch.Tensor\n",
        "            Updated node embeddings.\n",
        "        \"\"\"        \n",
        "\n",
        "        # Remove self loops\n",
        "        edge_index, _ = pyg_utils.remove_self_loops(edge_index)\n",
        "\n",
        "        if VERBOSE:\n",
        "            print('self.eps= ', self.eps)\n",
        "            print('num nodes=', x.size(0))\n",
        "            print_edge_index(edge_index)\n",
        "\n",
        "        x = self.lin_self(x) + self.propagate(edge_index, x=x, num_nodes=x.size(0))\n",
        "\n",
        "        return self.averaging(x)\n",
        "\n",
        "    def message(self, x_j, edge_index, num_nodes):\n",
        "        \"\"\"\n",
        "        Compute messages.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        ...\n",
        "        x_j : torch.Tensor\n",
        "            Source node embeddings of each edge. Meaning, x_j[17] will contain the features of the source node of edge #17.\n",
        "            To see the edge's nodes indices, see 'edge_index'\n",
        "            Has shape [NUM_EDGES, in_channels]\n",
        "        edge_index : torch.Tensor\n",
        "            Edge index. Has shape [2, NUM_EDGES]. The first entry in the first dimension is j \n",
        "            and the second entry is i. For example, if edge_index[0,17]=30 and edge_index[1,17]=21, \n",
        "            then there exists an edge from node 30 to node 21. The features data of node 30 will be\n",
        "            found in x_j[17], while the feature data of node 21 will be found in x_i[17].\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        res : torch.Tensor\n",
        "            The message from each source node to the target node, for each edge.\n",
        "            res[17] will contain the message from the source node of edge #17 to the target node\n",
        "            of edge #17. To see the edge's nodes indices, see 'edge_index'.\n",
        "            Has shape [NUM_EDGES, out_channels]\n",
        "        \"\"\"             \n",
        "\n",
        "        # This variable contains the first feature of the source node of each edge.\n",
        "        x_j_first_feature = torch.squeeze(torch.narrow(x_j, 1, 0, 1))\n",
        "        if VERBOSE:\n",
        "            print('x_j_first_feature=', x_j_first_feature)\n",
        "\n",
        "        # This variable will contain the order of the in-edges (of each node), \n",
        "        # sorted by the values of the first feature of the source nodes.\n",
        "        # For example, if neighborhood_ranking[17]=3, it means that edge #17 is \n",
        "        # in the 4th place, out of all of the edges with the same target node\n",
        "        # (when ordering these edges by the value of the first feature of \n",
        "        # their repective source nodes).\n",
        "        neighborhood_ranking = torch.zeros(x_j_first_feature.shape, dtype=torch.long)\n",
        "\n",
        "        # We iterate over each target node\n",
        "        for i in range(num_nodes):\n",
        "            # We first collect the indices of the in-edges of the target node\n",
        "            in_edges = torch.where(torch.narrow(edge_index, 0, 1, 1) == i)[1]\n",
        "            neighborhood_ranking[in_edges] = x_j_first_feature[in_edges].argsort(dim=0).argsort(dim=0)\n",
        "\n",
        "            if VERBOSE:\n",
        "                print('in-edges of node ' + str(i) + ':', ''.join([str(elem.item()) + ', ' for elem in in_edges]))\n",
        "\n",
        "        if VERBOSE:\n",
        "            print('neighborhood_ranking=', neighborhood_ranking)\n",
        "\n",
        "        # At this point we have the 'ranking' of every adjacent node. \n",
        "        # We apply the corresponding linear layer on all nodes with similar rankings. \n",
        "        res = torch.zeros((x_j.shape[0], self.out_channels))\n",
        "        for i in range(self.max_neighbors_to_consider):\n",
        "            indices = torch.where(neighborhood_ranking == i)\n",
        "            res[indices] = self.lin[i](x_j[indices])\n",
        "\n",
        "        return res\n",
        "\n",
        "    def averaging(self, x):\n",
        "        \"\"\"\n",
        "        Averages points along the first dimension. The purpose of this operation is to prevent \"jumping\" of the points\n",
        "        when we sort them along the first dimension. For each node, we take its epsilon-neighborhood, and we update the\n",
        "        feature of the node to be the average of the features of the nodes in its eplison-neighborhood.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        ...\n",
        "        x : torch.Tensor\n",
        "            The embedding of all nodes in the graph.\n",
        "            Has shape [NUM_NODES, channels]\n",
        "        \"\"\"   \n",
        "        \n",
        "        if VERBOSE:\n",
        "            print('x before averaging', x)\n",
        "\n",
        "        res = torch.zeros(x.shape)\n",
        "\n",
        "        first_dim_values, indices = torch.sort(torch.squeeze(torch.narrow(x, 1, 0, 1)))\n",
        "        for i, val in enumerate(first_dim_values):\n",
        "            eps_env = torch.where(abs(first_dim_values-val) < self.eps)\n",
        "            res[indices[i]] = torch.mean(x[indices[eps_env]], dim=0)\n",
        "\n",
        "        if VERBOSE:\n",
        "            print('x after averaging', res)\n",
        "\n",
        "        return res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Define the neural network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ymy1pgN5oNQG"
      },
      "outputs": [],
      "source": [
        "class GNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, task='node', use_custom_conv=False):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        task : str\n",
        "            Whether the task is to predict a label for each node (in the case of 'node'), or a label to the entire graph (in the case of 'graph').\n",
        "        use_custom_conv : boolean\n",
        "            Whether to use our custom convolution or the standard GCNConv of Torch Geometric.\n",
        "        \"\"\"\n",
        "        super(GNN, self).__init__()\n",
        "        self.task = task\n",
        "        self.use_custom_conv = use_custom_conv\n",
        "\n",
        "        # Note: this architechture is from some notebook I found https://colab.research.google.com/drive/1DIQm9rOx2mT1bZETEeVUThxcrP1RKqAn\n",
        "        # from some Stanford GNN course lecture https://www.youtube.com/watch?v=-UjytpbqX4A\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.convs.append(self.build_conv_model(input_dim, hidden_dim))\n",
        "        self.lns = nn.ModuleList()\n",
        "        self.lns.append(nn.LayerNorm(hidden_dim))\n",
        "        self.lns.append(nn.LayerNorm(hidden_dim))\n",
        "        for l in range(2):\n",
        "            self.convs.append(self.build_conv_model(hidden_dim, hidden_dim))\n",
        "\n",
        "        # post-message-passing\n",
        "        self.post_mp = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim), nn.Dropout(0.25),\n",
        "            nn.Linear(hidden_dim, output_dim))\n",
        "        if not (self.task == 'node' or self.task == 'graph'):\n",
        "            raise RuntimeError('Unknown task.')\n",
        "\n",
        "        self.dropout = 0.25\n",
        "        self.num_layers = 3\n",
        "\n",
        "    def build_conv_model(self, input_dim, hidden_dim):\n",
        "        if self.use_custom_conv:\n",
        "            return CustomConv(input_dim, hidden_dim)\n",
        "\n",
        "        # refer to pytorch geometric nn module for different implementation of GNNs.\n",
        "        if self.task == 'node':\n",
        "            return pyg_nn.GCNConv(input_dim, hidden_dim)\n",
        "        else:\n",
        "            return pyg_nn.GINConv(nn.Sequential(nn.Linear(input_dim, hidden_dim),\n",
        "                                  nn.ReLU(), nn.Linear(hidden_dim, hidden_dim)))\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "\n",
        "        if data.num_node_features == 0:\n",
        "          x = torch.ones(data.num_nodes, 1)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            # start = time.time()\n",
        "\n",
        "            x = self.convs[i](x, edge_index)\n",
        "            emb = x\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "            if not i == self.num_layers - 1:\n",
        "                x = self.lns[i](x)\n",
        "\n",
        "            # end = time.time()\n",
        "            # print(f'Finished layer {i}. Running time: {end - start:.2f} seconds')\n",
        "\n",
        "        if self.task == 'graph':\n",
        "            x = pyg_nn.global_mean_pool(x, batch)\n",
        "\n",
        "        x = self.post_mp(x)\n",
        "\n",
        "        return emb, F.log_softmax(x, dim=1)\n",
        "\n",
        "    def loss(self, pred, label):\n",
        "        return F.nll_loss(pred, label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bANNrQoh8xjF"
      },
      "source": [
        "# Training setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBwQxvFY83TG"
      },
      "source": [
        "We train the model in a standard way here, running it forwards to compute its predicted label distribution and backpropagating the error. Note the task setup in our graph setting: for node classification, we define a subset of nodes to be training nodes and the rest of the nodes to be test nodes, and mask out the test nodes during training via `batch.train_mask`. For graph classification, we use 80% of the graphs for training and the remainder for testing, as in other classification settings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "u5nqB3HHoHc7"
      },
      "outputs": [],
      "source": [
        "def train(dataset, task, use_custom = False):\n",
        "    if task == 'graph':\n",
        "        data_size = len(dataset)\n",
        "        loader = DataLoader(dataset[:int(data_size * 0.8)], batch_size=64, shuffle=True)\n",
        "        test_loader = DataLoader(dataset[int(data_size * 0.8):], batch_size=64, shuffle=True)\n",
        "\n",
        "    else:\n",
        "        test_loader = loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "        \n",
        "\n",
        "    # build model\n",
        "    model = GNN(max(dataset.num_node_features, 1), 32, dataset.num_classes, task=task, use_custom_conv=use_custom)\n",
        "    opt = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "    # train\n",
        "    for epoch in range(101):\n",
        "        if VERBOSE:\n",
        "            print('epoch #' + str(epoch))\n",
        "        total_loss = 0\n",
        "        model.train()\n",
        "        for batch in loader:\n",
        "            opt.zero_grad()\n",
        "            embedding, pred = model(batch)\n",
        "            label = batch.y\n",
        "            if task == 'node':\n",
        "                pred = pred[batch.train_mask]\n",
        "                label = label[batch.train_mask]\n",
        "            loss = model.loss(pred, label)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            total_loss += loss.item() * batch.num_graphs\n",
        "        total_loss /= len(loader.dataset)\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            test_acc = test(test_loader, model, True)\n",
        "            print(\"Epoch {}. Loss: {:.4f}. Test accuracy: {:.4f}\".format(\n",
        "                epoch, total_loss, test_acc))\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rC8IPZSOXraQ"
      },
      "source": [
        "Test time, for the CiteSeer/Cora node classification task, there is only 1 graph. So we use masking to determine validation and test set.\n",
        "\n",
        "For graph classification tasks, a subset of graphs is considered validation / test graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "KvUBHtZaXo2h"
      },
      "outputs": [],
      "source": [
        "def test(loader, model, is_train=False):\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    for data in loader:\n",
        "        with torch.no_grad():\n",
        "            emb, pred = model(data)\n",
        "            pred = pred.argmax(dim=1)\n",
        "            label = data.y\n",
        "\n",
        "        if model.task == 'node':\n",
        "            mask = data.train_mask if is_train else data.test_mask\n",
        "            # node classification: only evaluate on nodes in test set\n",
        "            pred = pred[mask]\n",
        "            label = data.y[mask]\n",
        "\n",
        "        correct += pred.eq(label).sum().item()\n",
        "\n",
        "    if model.task == 'graph':\n",
        "        total = len(loader.dataset)\n",
        "    else:\n",
        "        total = 0\n",
        "        for data in loader.dataset:\n",
        "            mask = data.train_mask if is_train else data.test_mask\n",
        "            total += torch.sum(mask).item()\n",
        "    return correct / total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUo2Ve8c9wGp"
      },
      "source": [
        "# Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "wf4-g8wT-qsj"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading https://data.pyg.org/datasets/benchmarking-gnns/MNIST_v2.zip\n",
            "Extracting /tmp/MNIST/MNIST/raw/MNIST_v2.zip\n",
            "Processing...\n",
            "Done!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0. Loss: 1.9695. Test accuracy: 0.2705\n",
            "Epoch 10. Loss: 1.8795. Test accuracy: 0.3547\n",
            "Epoch 20. Loss: 1.8044. Test accuracy: 0.2196\n",
            "Epoch 30. Loss: 1.8354. Test accuracy: 0.3343\n",
            "Epoch 40. Loss: 1.8297. Test accuracy: 0.3317\n",
            "Epoch 50. Loss: 1.8102. Test accuracy: 0.3738\n",
            "Epoch 60. Loss: 1.7895. Test accuracy: 0.3854\n",
            "Epoch 70. Loss: 1.8148. Test accuracy: 0.3450\n"
          ]
        }
      ],
      "source": [
        "dataset = GNNBenchmarkDataset(root='/tmp/MNIST', name='MNIST')\n",
        "dataset = dataset.shuffle()\n",
        "task = 'graph'\n",
        "\n",
        "model = train(dataset, task)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And now with our custom model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = train(dataset, task, use_custom=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we try a node classification task on the Citeseer citation network:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# dataset = FakeDataset(num_graphs=200, avg_num_nodes=500, num_channels=16, avg_degree=4)\n",
        "# task = 'graph'\n",
        "\n",
        "# model = train(dataset, task)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And now with our custom model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model = train(dataset, task, use_custom=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
